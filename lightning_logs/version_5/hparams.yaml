activation: relu
criterion: null
dropout: 0.0
ffn_hidden_dim: 300
ffn_num_layers: 1
final_lr: 0.0001
init_lr: 0.0001
max_lr: 0.001
metrics: null
n_tasks: 1
num_lrs: 1
scaler: null
task_weights: null
warmup_epochs: 2
